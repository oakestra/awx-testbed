- name: Deploy root and cluster components for MDOC topology
  hosts: "{{ group_rc_together }}"
  gather_facts: false
  vars:
    repo: "{{ oak_repo_link }}"
    path: "{{ oak_repo_path }}"
    branch: "{{ oak_repo_branch }}"
    commit: "{{ oak_repo_commit }}"

  tasks:
    - name: Set OAK_STATUS and OAK_ROLE by including role
      include_role:
        name: set-role-availability
      vars:
        role: "root"
        status: "busy"

    - name: Include role to ensure cloned repo
      include_role:
        name: ensure-oakestra-repo-is-cloned
      vars:
        path: "{{ oak_repo_path }}"
        repo: "{{ oak_repo_link }}"
        branch: "{{ oak_repo_branch }}"
        commit: "{{ oak_repo_commit }}"

    - name: Retrieve rootIP address from hostname
      set_fact:
        root_cluster_ip: "{{ hostvars[inventory_hostname].ansible_host | default(hostvars[inventory_hostname].ansible_ssh_host, 'IP address not found') }}"

    - name: Check root ip validity
      fail:
        msg: "IP address not found for root node {{ inventory_hostname }}"
      when: root_cluster_ip == "IP address not found"

    - name: Include role to run root component for node {{ inventory_hostname }}
      include_role:
        name: run-root
      vars:
        rootIP: "{{ root_cluster_ip }}"

    - name: Run cluster entrypoint for node {{ inventory_hostname }}
      include_role:
        name: run-cluster
      vars:
        rootIP: "{{ root_cluster_ip }}"
        clusterName: "CL{{ inventory_hostname }}"
        clusterLocation: "Garching2"

    - name: Set root cluster IP for worker nodes
      set_fact:
        root_cluster_ip: "{{ root_cluster_ip }}"

- name: Deploy worker components for full topology
  hosts: "{{ group_rc_workers }}"
  tasks:

    - name: Retrieve root&cluster hostname from group_rc_together
      set_fact:
        root_hostname: "{{ group_rc_together[0] }}"

    - name: Retrieve root IP address
      set_fact:
        root_cluster_ip: "{{ hostvars[root_hostname].ansible_host | default(hostvars[root_hostname].ansible_ssh_host, 'IP address not found') }}"

    - name: Print root&cluster IP address for worker nodes
      debug:
        msg: "Root&Cluster IP address for worker {{ inventory_hostname }} is {{ root_cluster_ip }}"

    - name: Set OAK_STATUS and OAK_ROLE by including role
      include_role:
        name: set-role-availability
      vars:
        role: "worker_{{ inventory_hostname }}"
        status: "busy"

    - name: Setup network manager entrypoint for node {{ inventory_hostname }}
      include_role:
        name: setup-net-manager
      vars:
        nodeIP: "{{ root_cluster_ip }}"
        clusterIP: "{{ root_cluster_ip }}"
        path: "{{ oak_net_repo_path }}"
        repo: "{{ oak_net_repo_link }}"
        branch: "{{ oak_net_repo_branch }}"
        commit: "{{ oak_net_repo_commit }}"

    - name: Ensure NetManager service is running on node {{ inventory_hostname }}
      systemd:
        name: netmanager
        state: started
        enabled: true
        daemon_reload: true
      become: true
      ignore_errors: false

    - name: Setup node engine entrypoint for node {{ inventory_hostname }}
      include_role:
        name: setup-node-engine
      vars:
        nodeIP: "{{ root_cluster_ip }}"
        clusterIP: "{{ root_cluster_ip }}"
        path: "{{ oak_repo_path }}"
        repo: "{{ oak_repo_link }}"
        branch: "{{ oak_repo_branch }}"
        commit: "{{ oak_repo_commit }}"

    - name: Start NodeEngine on {{ inventory_hostname }}
      environment:
        CLUSTER_IP: '{{ root_cluster_ip }}'
      shell: 'NodeEngine -n 6000 -a $CLUSTER_IP -p 10100 -d &'
      become: true
      ignore_errors: false

    - name: Ensure systemctl nodeengine service is running on node {{ inventory_hostname }}
      systemd:
        name: nodeengine
        state: started
        enabled: true
        daemon_reload: true
      become: true
      ignore_errors: false
