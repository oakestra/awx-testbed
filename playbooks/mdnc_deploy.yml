- name: Execute Full Deploy template
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Check MNDC Deploy flags
      block:
        - name: Show groups division
          debug:
            msg: |
              Root hosts: {{ group_root_full }}
              Cluster hosts: {{ group_clusters_full }}
              Workers hosts: {{ group_workers_full }}

        - name: Parsed topology descriptor
          debug:
            msg: "{{ topology_desc }}"

        - name: Execute the support Script
          command: >
            /usr/bin/python3 "{{ testbed_tmp_path }}"/utils/mdnc_deploy/combination.py "{{ testbed_tmp_path }}/topologies/{{ topology_descriptor }}" "{{ group_clusters_full }}" "{{ group_workers_full }}"
          register: result_output

        - name: Set dictionary result output
          ansible.builtin.set_fact:
            script_result: "{{ result_output.stdout }}"

        - name: Debug the script result
          debug:
            msg: "script_result is {{ script_result }}"

        - name: Set topology_desc as JSON
          set_fact:
            topology_desc: "{{ topology_desc | to_json }}"

        - name: Set facts for cluster and worker associations
          set_fact:
            clusters: "{{ topology_desc.topology_descriptor.cluster_list }}"

        - name: Set cluster IDs
          set_fact:
            cluster_ids: "{{ topology_desc.topology_descriptor.cluster_list | map(attribute='cluster_number') | list }}"

        - name: Cast num_clusters to an integer
          set_fact:
            num_clusters_int: "{{ num_clusters | int }}"

        - name: Set cluster host ranges
          set_fact:
            cluster_host_ranges: >
              {{ dict(cluster_ids | zip(reserved_hosts[1:(1 | int + (num_clusters_int | int))]) | list) }}
          register: cluster_host_ranges

        - name: Set worker map
          set_fact:
            worker_map: "{{ hostvars['localhost'].script_result }}"
          register: worker_map

        - name: Set cluster map
          set_fact:
            cluster_map: "{{ cluster_host_ranges.ansible_facts.cluster_host_ranges }}"
          register: cluster_map

        - name: Print the cluster and worker associations
          debug:
            msg: |
              Cluster Host Map: {{ cluster_host_ranges.ansible_facts.cluster_host_ranges }}
              Worker Map: {{ worker_map }}
      when: not onedoc and not mdoc

################################################## DEPLOY: ROOT ##################################################
- name: Deploy root component for full topology
  hosts: "{{ group_root_full }}"
  vars:
    repo: "{{ oak_repo_link }}"
    path: "{{ oak_repo_path }}"
    branch: "{{ oak_repo_branch }}"
    commit: "{{ oak_repo_commit }}"

  tasks:
    - name: Set OAK_STATUS and OAK_ROLE by including role
      include_role:
        name: set-role-availability
      vars:
        role: "root"
        status: "busy"

    - name: Set environment variables for deployment root (node {{ inventory_hostname }})
      set_fact:
        cluster_map: "{{ hostvars['localhost'].cluster_map.ansible_facts.cluster_map }}"
        worker_map: "{{ hostvars['localhost'].worker_map.ansible_facts.worker_map }}"

    - name: Print the cluster and worker IPs at root 
      debug:
        msg: |
          Cluster IPs: "{{ cluster_map }}"
          Worker IPs: "{{ worker_map }}"

    - name: Include role to ensure cloned repo
      include_role:
        name: ensure-oakestra-repo-is-cloned
      vars:
        path: "{{ oak_repo_path }}"
        repo: "{{ oak_repo_link }}"
        branch: "{{ oak_repo_branch }}"
        commit: "{{ oak_repo_commit }}"

    - name: Retrieve rootIP address from hostname
      set_fact:
        root_ip: "{{ hostvars[inventory_hostname].ansible_host | default(hostvars[inventory_hostname].ansible_ssh_host, 'IP address not found') }}"
    
    - name: Check root ip validity
      fail:
        msg: "IP address not found for root node {{ inventory_hostname }}"
      when: root_ip == "IP address not found"

    - name: Include role to run root component for node {{ inventory_hostname }}
      include_role:
        name: run-root
      vars:
        rootIP: "{{ root_ip }}"

################################################## DEPLOY: CLUSTER ##################################################

- name: Deploy cluster components for full topology
  hosts: "{{ group_clusters_full }}"

  tasks:
    - name: Set OAK_STATUS and OAK_ROLE by including role
      include_role:
        name: set-role-availability
      vars:
        role: "cluster_{{ inventory_hostname }}"
        status: "busy"

    - name: Set environment variables for deployment cluster
      set_fact:
        cluster_map: "{{ hostvars['localhost'].cluster_map.ansible_facts.cluster_map }}"
        worker_map: "{{ hostvars['localhost'].worker_map.ansible_facts.worker_map }}"

    - name: Print the cluster and worker IPs at cluster
      debug:
        msg: |
          Cluster IPs: "{{ cluster_map }}"
          Worker IPs: "{{ worker_map }}"

    - name: Retrieve root hostname from group_root_full
      set_fact:
        root_name: "{{ group_root_full[0] }}"

    - name: Retrieve root IP address
      set_fact:
        root_ip: "{{ hostvars[root_name].ansible_host | default(hostvars[root_name].ansible_ssh_host, 'IP address not found') }}"

    - name: Print root IP address for cluster
      debug:
        msg: "Root IP address for cluster {{ inventory_hostname }} is {{ root_ip }}"

    - name: Ensure cloned repo for node {{ inventory_hostname }}
      include_role:
        name: ensure-oakestra-repo-is-cloned
      vars:
        path: "{{ oak_repo_path }}"
        repo: "{{ oak_repo_link }}"
        branch: "{{ oak_repo_branch }}"
        commit: "{{ oak_repo_commit }}"

    - name: Run cluster entrypoint for node {{ inventory_hostname }}
      include_role:
        name: run-cluster
      vars:
        rootIP: "{{ root_ip }}"
        clusterName: "CL{{ inventory_hostname }}"
        clusterLocation: "Garching2"

################################################## DEPLOY: WORKER ##################################################

- name: Deploy worker components for full topology
  hosts: "{{ group_workers_full }}"
  tasks:
    - name: Set OAK_STATUS and OAK_ROLE by including role
      include_role:
        name: set-role-availability
      vars:
        role: "worker_{{ inventory_hostname }}"
        status: "busy"

    - name: Set environment variables for deployment node (worker {{ inventory_hostname }})
      set_fact:
        cluster_map: "{{ hostvars['localhost'].cluster_map.ansible_facts.cluster_map}}"
        worker_map: "{{ hostvars['localhost'].worker_map.ansible_facts.worker_map }}"

    - name: Show cluster and worker IPs at node
      debug:
        msg: |
          Cluster IPs: "{{ cluster_map }}"
          Worker IPs: "{{ worker_map }}"


    - name: Show cluster_node_map association
      debug:
        msg: "{{ cluster_map }}"
      
    - name: Determine cluster for current worker and print message
      debug:
        msg: >
          {% set cluster_id = worker_map | dict2items | selectattr('value', 'contains', inventory_hostname) | map(attribute='key') | first %}
          {% if cluster_id is defined %}
            {% set cluster_node_name = cluster_map[(cluster_id | int )] %}
            Hey, I'm host {{ inventory_hostname }} and I'm inside cluster of node {{ cluster_node_name }}
          {% else %}
            Cluster information not found for host {{ inventory_hostname }}
          {% endif %}
    
    - name: Set cluster id for worker
      set_fact:
        cluster_id: "{{ worker_map | dict2items | selectattr('value', 'contains', inventory_hostname) | map(attribute='key') | first }}"

    - name: Set cluster node name for worker
      set_fact:
        cluster_node_name: "{{ cluster_map[(cluster_id | int)] }}"

    - name: Retrieve IP address of cluster node
      set_fact:
        cluster_node_ip: "{{ hostvars[cluster_node_name].ansible_host | default(hostvars[cluster_node_name].ansible_ssh_host, 'IP address not found') }}"

    - name: Check if cluster_node_ip is valid
      fail:
        msg: "IP address not found for cluster node {{ cluster_node_name }}"
      when: cluster_node_ip == "IP address not found"
    
    - name: Show node-cluster association
      debug:
        msg: "Cluster node IP for worker {{ inventory_hostname }} is {{ cluster_node_ip }}"

    - name: Setup network manager entrypoint for node {{ inventory_hostname }}
      include_role:
        name: setup-net-manager
      vars:
        nodeIP: "{{ cluster_node_ip }}"
        clusterIP: "{{ cluster_node_ip }}"
        path: "{{ oak_net_repo_path }}"
        repo: "{{ oak_net_repo_link }}"
        branch: "{{ oak_net_repo_branch }}"
        commit: "{{ oak_net_repo_commit }}"

    - name: Ensure NetManager service is running on node {{ inventory_hostname }}
      systemd:
        name: netmanager
        state: started
        enabled: true
        daemon_reload: true
      become: true
      ignore_errors: false

    - name: Setup node engine entrypoint for node {{ inventory_hostname }}
      include_role:
        name: setup-node-engine
      vars:
        nodeIP: "{{ cluster_node_ip }}"
        clusterIP: "{{ cluster_node_ip }}"
        path: "{{ oak_repo_path }}"
        repo: "{{ oak_repo_link }}"
        branch: "{{ oak_repo_branch }}"
        commit: "{{ oak_repo_commit }}"

    - name: Start NodeEngine on {{ inventory_hostname }}
      environment:
        CLUSTER_IP: '{{ cluster_node_ip }}'
      shell: 'NodeEngine -n 6000 -a $CLUSTER_IP -p 10100 -d &'
      become: true
      ignore_errors: false

    - name: Ensure systemctl nodeengine service is running on node {{ inventory_hostname }}
      systemd:
        name: nodeengine
        state: started
        enabled: true
        daemon_reload: true
      become: true
      ignore_errors: false
