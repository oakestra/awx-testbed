- name: (MDNC) Deploy template
  hosts: localhost
  gather_facts: false
  vars:
    repo: "https://github.com/TheDarkPyotr/sequential-test"
    branch: "main"
  tasks:
    - name: Check MNDC Deploy flags
      block:
        - name: Show groups division
          debug:
            msg: |
              input_data.group_mdnc_root: {{ input_data.group_mdnc_root }}
              input_data.group_mdnc_clusters: {{ input_data.group_mdnc_clusters }}
              input_data.group_mdnc_workers: {{ input_data.group_mdnc_workers }}

        - name: Execute the support Script
          command: >
            /usr/bin/python3 "{{ testbed_tmp_path }}"/utils/mdnc_deploy/combination.py "{{ testbed_tmp_path }}/{{ topologies_folder }}/{{ input_data.topology_filename }}" "{{ input_data.group_mdnc_clusters }}" "{{ input_data.group_mdnc_workers }}"
          register: result_output

        - name: Set dictionary result output
          ansible.builtin.set_fact:
            script_result: "{{ result_output.stdout }}"

        - name: Debug the script result
          debug:
            msg: "script_result is {{ script_result }}"

        - name: Read  topology descriptor JSON file
          ansible.builtin.slurp:
            src: "{{ testbed_tmp_path }}/{{ topologies_folder }}/{{ input_data.topology_filename }}"
          register: json_file_content

        - name: Decode JSON content
          set_fact:
            topology_descriptor_content: "{{ json_file_content.content | b64decode }}"
            
        - name: Debug the type of decoded content
          debug:
            msg: "Type of topology_descriptor_content: {{ topology_descriptor_content.__class__.__name__ }}"

        - name: Set topology_desc as a parsed JSON object if content is a string
          set_fact:
            topology_desc: "{{ topology_descriptor_content if topology_descriptor_content is not string else topology_descriptor_content | from_json }}"

        - name: Print the topology descriptor
          debug:
            msg: "{{ topology_desc }}"

        - name: Set facts for cluster and worker associations
          set_fact:
            clusters: "{{ topology_desc.topology_descriptor.cluster_list }}"

        - name: Set cluster IDs
          set_fact:
            cluster_ids: "{{ topology_desc.topology_descriptor.cluster_list | map(attribute='cluster_number') | list }}"

        - name: Set num_clusters as the length of cluster_ids
          set_fact:
            num_clusters: "{{ cluster_ids | length }}"

        - name: Cast num_clusters to an integer
          set_fact:
            num_clusters_int: "{{ num_clusters | int }}"

        - name: Set cluster host ranges
          set_fact:
            cluster_host_ranges: >
              {{ dict(cluster_ids | zip(input_data.group_mdnc_clusters[0:(1 | int + (num_clusters_int | int))]) | list) }}
          register: cluster_host_ranges

        - name: Set worker map
          set_fact:
            worker_map: "{{ hostvars['localhost'].script_result }}"
          register: worker_map

        - name: Set cluster map
          set_fact:
            cluster_map: "{{ cluster_host_ranges.ansible_facts.cluster_host_ranges }}"
          register: cluster_map

        - name: Print the cluster and worker associations
          debug:
            msg: |
              Cluster Host Map: {{ cluster_map }}
              Worker Map: {{ worker_map }}
      when: not onedoc and not mdoc

################################################## DEPLOY: ROOT ##################################################
- name: Deploy root component for full topology
  hosts: "{{ input_data.group_mdnc_root }}"
  vars:
    repo: "{{ oak_repo_link }}"
    path: "{{ oak_repo_path }}"
    branch: "{{ oak_repo_version }}"
    commit: "{{ oak_repo_commit }}"


  tasks:
    - name: Set OAK_STATUS and OAK_ROLE by including role
      include_role:
        name: set-role-availability
      vars:
        role: "root"
        status: "busy"

    - name: Set environment variables for deployment root (node {{ inventory_hostname }})
      set_fact:
        cluster_map: "{{ hostvars['localhost'].cluster_map.ansible_facts.cluster_map }}"
        worker_map: "{{ hostvars['localhost'].worker_map.ansible_facts.worker_map }}"

    - name: Print the cluster and worker IPs at root 
      debug:
        msg: |
          Cluster IPs: "{{ cluster_map }}"
          Worker IPs: "{{ worker_map }}"

    - name: Include role to ensure cloned repo
      include_role:
        name: ensure-oakestra-repo-is-cloned
      vars:
        path: "{{ oak_repo_path }}"
        repo: "{{ oak_repo_link }}"
        branch: "{{ oak_repo_version }}"
        commit: "{{ oak_repo_commit }}"


    - name: Retrieve rootIP address from hostname
      set_fact:
        root_ip: "{{ hostvars[inventory_hostname].ansible_host | default(hostvars[inventory_hostname].ansible_ssh_host, 'IP address not found') }}"
    
    - name: Check root ip validity
      fail:
        msg: "IP address not found for root node {{ inventory_hostname }}"
      when: root_ip == "IP address not found"

    - name: Include role to run root component for node {{ inventory_hostname }}
      include_role:
        name: run-root
      vars:
        rootIP: "{{ root_ip }}"

################################################## DEPLOY: CLUSTER ##################################################

- name: Deploy cluster components for full topology
  hosts: "{{ input_data.group_mdnc_clusters }}"

  tasks:
    - name: Set OAK_STATUS and OAK_ROLE by including role
      include_role:
        name: set-role-availability
      vars:
        role: "cluster_{{ inventory_hostname }}"
        status: "busy"

    - name: Set environment variables for deployment cluster
      set_fact:
        cluster_map: "{{ hostvars['localhost'].cluster_map.ansible_facts.cluster_map }}"
        worker_map: "{{ hostvars['localhost'].worker_map.ansible_facts.worker_map }}"

    - name: Print the cluster and worker IPs at cluster
      debug:
        msg: |
          Cluster IPs: "{{ cluster_map }}"
          Worker IPs: "{{ worker_map }}"

    - name: Retrieve root hostname from input_data.group_mdnc_root
      set_fact:
        root_name: "{{ input_data.group_mdnc_root[0] }}"

    - name: Retrieve root IP address
      set_fact:
        root_ip: "{{ hostvars[root_name].ansible_host | default(hostvars[root_name].ansible_ssh_host, 'IP address not found') }}"

    - name: Print root IP address for cluster
      debug:
        msg: "Root IP address for cluster {{ inventory_hostname }} is {{ root_ip }}"

    - name: Ensure cloned repo for node {{ inventory_hostname }}
      include_role:
        name: ensure-oakestra-repo-is-cloned
      vars:
        path: "{{ oak_repo_path }}"
        repo: "{{ oak_repo_link }}"
        branch: "{{ oak_repo_version }}"
        commit: "{{ oak_repo_commit }}"


    - name: Run cluster entrypoint for node {{ inventory_hostname }}
      include_role:
        name: run-cluster
      vars:
        rootIP: "{{ root_ip }}"
        clusterName: "CL{{ inventory_hostname }}"
        clusterLocation: "Garching2"

################################################## DEPLOY: WORKER ##################################################

- name: Deploy worker components for full topology
  hosts: "{{ input_data.group_mdnc_workers }}"
  tasks:
    - name: Set OAK_STATUS and OAK_ROLE by including role
      include_role:
        name: set-role-availability
      vars:
        role: "worker_{{ inventory_hostname }}"
        status: "busy"

    - name: Print llocalhost hostvars
      debug:
        msg: |
          Cluster MAP: "{{ hostvars['localhost'].cluster_map.ansible_facts.cluster_map }}"
          Worker MAP: "{{ hostvars['localhost'].worker_map.ansible_facts.worker_map }}"

    - name: Set environment variables for deployment node (worker {{ inventory_hostname }})
      set_fact:
        cluster_map: "{{ hostvars['localhost'].cluster_map.ansible_facts.cluster_map}}"
        worker_map: "{{ hostvars['localhost'].worker_map.ansible_facts.worker_map }}"

    - name: Show cluster and worker IPs at node
      debug:
        msg: |
          Cluster IPs: "{{ cluster_map }}"
          Worker IPs: "{{ worker_map }}"


    - name: Show cluster_node_map association
      debug:
        msg: "{{ cluster_map }}"
      
    - name: Determine cluster for current worker and print message
      debug:
        msg: >
          {% set cluster_id = worker_map | dict2items | selectattr('value', 'contains', inventory_hostname) | map(attribute='key') | first %}
          {% if cluster_id is defined %}
            {% set cluster_node_name = cluster_map[(cluster_id | int )] %}
            Hey, I'm host {{ inventory_hostname }} and I'm inside cluster of node {{ cluster_node_name }}
          {% else %}
            Cluster information not found for host {{ inventory_hostname }}
          {% endif %}
    
    - name: Set cluster id for worker
      set_fact:
        cluster_id: "{{ worker_map | dict2items | selectattr('value', 'contains', inventory_hostname) | map(attribute='key') | first }}"

    - name: Set cluster node name for worker
      set_fact:
        cluster_node_name: "{{ cluster_map[(cluster_id | int)] }}"

    - name: Retrieve IP address of cluster node
      set_fact:
        cluster_node_ip: "{{ hostvars[cluster_node_name].ansible_host | default(hostvars[cluster_node_name].ansible_ssh_host, 'IP address not found') }}"

    - name: Check if cluster_node_ip is valid
      fail:
        msg: "IP address not found for cluster node {{ cluster_node_name }}"
      when: cluster_node_ip == "IP address not found"
    
    - name: Show node-cluster association
      debug:
        msg: "Cluster node IP for worker {{ inventory_hostname }} is {{ cluster_node_ip }}"

    - name: Setup network manager entrypoint for node {{ inventory_hostname }}
      include_role:
        name: setup-net-manager
      vars:
        nodeIP: "{{ cluster_node_ip }}"
        clusterIP: "{{ cluster_node_ip }}"
        path: "{{ oak_repo_path }}"
        repo: "{{ oak_repo_link }}"
        branch: "{{ oak_repo_version }}"

    - name: Restart Net Manager
      shell: 'nohup /bin/NetManager -p 6000 </dev/null >/tmp/netmanager.log 2>&1 &'
      become: true
      ignore_errors: false

    - name: Setup node engine entrypoint for node {{ inventory_hostname }}
      include_role:
        name: setup-node-engine
      vars:
        nodeIP: "{{ cluster_node_ip }}"
        clusterIP: "{{ cluster_node_ip }}"
        path: "{{ oak_repo_path }}"
        repo: "{{ oak_repo_link }}"
        branch: "{{ oak_repo_version }}"

    - name: Run node-engine on node {{ inventory_hostname }}
      environment:
        CLUSTER_IP: '{{ cluster_node_ip }}'
      shell: 'nohup /bin/NodeEngine -n 6000 -a $CLUSTER_IP -p 10100 </dev/null >/tmp/nodeengine.log 2>&1 &'
      become: true
      ignore_errors: false

- name: Define stats for component status check
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Read {{ input_data.topology_filename  }} topology descriptor JSON file
      ansible.builtin.slurp:
        src: "{{ testbed_tmp_path }}/{{ topologies_folder }}/{{ input_data.topology_filename }}"
      register: json_file_content

    - name: Decode {{ input_data.topology_filename  }} JSON content
      set_fact:
        topology_descriptor: "{{ json_file_content.content | b64decode }}"

    - name: Set stats for component status check
      set_stats:
        data:
          group_1doc: "[]"
          group_mdoc_root: "[]"
          group_mdoc_workers: "[]"
          group_mdnc_root: "{{ input_data.group_mdnc_root }}"
          group_mdnc_clusters: "{{ input_data.group_mdnc_clusters }}"
          group_mdnc_workers: "{{ input_data.group_mdnc_workers }}"
          onedoc: false
          mdoc: false
          topology_desc: "{{ topology_descriptor }}"